# üìä deces-dataprep

Projet de pr√©paration et d'indexation des donn√©es du fichier des personnes d√©c√©d√©es de l'INSEE. Ce projet utilise l'√©cosyst√®me [matchID](https://github.com/matchid-project) pour transformer et indexer les donn√©es de d√©c√®s dans Elasticsearch.

üîó **Donn√©es source** : [Fichier des personnes d√©c√©d√©es sur data.gouv.fr](https://www.data.gouv.fr/fr/datasets/fichier-des-personnes-decedees/)

---

## üìã Table des mati√®res

- [Vue d'ensemble](#-vue-densemble)
- [Pr√©requis](#-pr√©requis)
- [Installation](#-installation)
- [D√©marrage rapide](#-d√©marrage-rapide)
- [Workflows](#-workflows)
- [Commandes disponibles](#-commandes-disponibles)
- [Configuration](#-configuration)
- [Traitements des donn√©es](#-traitements-des-donn√©es)
- [D√©ploiement distant](#-d√©ploiement-distant)
- [D√©pannage](#-d√©pannage)
- [License](#-license)

---

## üèóÔ∏è Vue d'ensemble

Le projet [`deces-dataprep`](.) orchestre le pipeline complet de traitement des donn√©es de d√©c√®s depuis data.gouv.fr jusqu'√† la cr√©ation d'index Elasticsearch optimis√©s pour la recherche.

### Architecture du pipeline

```
Data.gouv.fr ‚Üí Stockage S3 ‚Üí Traitement matchID ‚Üí Index Elasticsearch ‚Üí Sauvegarde
```

### Composants principaux

| Composant | Description | Fichier |
|-----------|-------------|---------|
| **Source** | Fichiers texte √† largeur fixe de l'INSEE | [`deces_src.yml`](projects/deces-dataprep/datasets/deces_src.yml:1) |
| **Recette** | Pipeline de transformation des donn√©es | [`deces_dataprep.yml`](projects/deces-dataprep/recipes/deces_dataprep.yml:1) |
| **Destination** | Index Elasticsearch avec mappings optimis√©s | [`deces_index.yml`](projects/deces-dataprep/datasets/deces_index.yml:1) |
| **Orchestration** | Automatisation via Makefile | [`Makefile`](Makefile:1) |

### Traitements appliqu√©s

- ‚úÖ Normalisation des noms et pr√©noms (casse, format)
- ‚úÖ Validation et correction des dates de naissance et d√©c√®s
- ‚úÖ Enrichissement g√©ographique (communes, d√©partements, pays)
- ‚úÖ Filtrage des oppositions RGPD
- ‚úÖ Calcul de l'√¢ge au d√©c√®s
- ‚úÖ Historique des codes INSEE (fusions de communes)
- ‚úÖ G√©ocodage (coordonn√©es GPS)
- ‚úÖ Mapping des anciennes colonies fran√ßaises

---

## ‚úÖ Pr√©requis

### Logiciels requis

- **Docker** (‚â• 20.10) et **Docker Compose** (‚â• 2.0)
- **Git** (‚â• 2.0)
- **Make** (GNU Make ‚â• 4.0)
- **Bash** (shell par d√©faut)

### Ressources syst√®me recommand√©es

| Environnement | RAM | Espace disque | CPU |
|---------------|-----|---------------|-----|
| **D√©veloppement** | 8 GB | 20 GB | 4 c≈ìurs |
| **Production** | 16 GB | 100 GB | 8 c≈ìurs |

### Ports utilis√©s

- **8081** : Interface MatchID (frontend)
- **9200** : Elasticsearch
- **5000** : API Backend MatchID

### Acc√®s S3

Le projet supporte deux connecteurs S3 :

- **D√©veloppement local** : Utilise le backend int√©gr√© ou un stockage local
- **Production** : N√©cessite des cl√©s d'acc√®s S3 (Scaleway, AWS, etc.)

---

## üîß Installation

### 1. Cloner le projet

```bash
git clone https://github.com/matchid-project/deces-dataprep.git
cd deces-dataprep
```

### 2. Configuration initiale

```bash
make config
```

Cette commande :
- Clone le [backend matchID](https://github.com/matchid-project/backend) dans le r√©pertoire [`backend/`](Makefile:136)
- Copie les fichiers de configuration ([`artifacts`](Makefile:139), [`docker-compose-local.yml`](Makefile:140))
- Configure les variables d'environnement
- V√©rifie les pr√©requis syst√®me

### 3. Configuration S3 (optionnel)

Pour utiliser un stockage S3 externe :

```bash
export STORAGE_ACCESS_KEY=votre_cle_acces
export STORAGE_SECRET_KEY=votre_cle_secrete
export REPOSITORY_BUCKET=votre-bucket-elasticsearch
```

---

## üöÄ D√©marrage rapide

### D√©veloppement local

```bash
# D√©marrer l'environnement complet
make dev

# Acc√®s aux services :
# - Interface MatchID : http://localhost:8081
# - Elasticsearch : http://localhost:9200
```

L'interface MatchID permet de :
- Visualiser et tester les recettes de traitement
- Monitorer l'avancement du traitement
- Inspecter les donn√©es transform√©es
- D√©boguer les transformations

### Traitement complet automatique

```bash
# Traitement de bout en bout
make all
```

Cette commande ex√©cute s√©quentiellement :
1. **Configuration** ([`all-step0`](Makefile:278)) : Initialisation de l'environnement
2. **Traitement** ([`all-step1`](Makefile:281)) : Synchronisation et transformation des donn√©es
3. **Surveillance** ([`watch-run`](Makefile:191)) : Monitoring en temps r√©el
4. **Sauvegarde** ([`all-step2`](Makefile:284)) : Backup vers le stockage S3

‚è±Ô∏è **Dur√©e estim√©e** : 1h30 √† 10h selon la taille des donn√©es et les ressources syst√®me.

### Arr√™t des services

```bash
# Arr√™t gracieux
make dev-stop

# Arr√™t complet avec nettoyage
make down
```

---

## üîÑ Workflows

### Workflow de d√©veloppement

```bash
# 1. D√©marrer l'environnement de d√©veloppement
make dev

# 2. Acc√©der √† l'interface MatchID
# http://localhost:8081

# 3. Tester une recette sur un √©chantillon de donn√©es
# (via l'interface ou les commandes backend)

# 4. Arr√™ter l'environnement
make dev-stop
```

### Workflow de traitement complet

```bash
# 1. Synchroniser les donn√©es depuis data.gouv.fr
make datagouv-to-storage

# 2. G√©n√©rer le tag de version des donn√©es
make data-tag

# 3. V√©rifier si un backup existe d√©j√†
make repository-check

# 4. Lancer le traitement (si pas de backup existant)
make recipe-run

# 5. Surveiller l'avancement
make watch-run

# 6. Sauvegarder le r√©sultat
make repository-push
```

### Workflow de restauration

```bash
# 1. V√©rifier les backups disponibles
make repository-check

# 2. Restaurer depuis le backup
make repository-restore

# 3. D√©marrer les services
make up

# 4. V√©rifier le nombre d'enregistrements
docker exec -it matchid-elasticsearch curl localhost:9200/_cat/indices
```

### Workflow de mise √† jour incr√©mentale

```bash
# 1. Synchroniser uniquement les nouveaux fichiers
make datagouv-to-storage

# 2. V√©rifier la version des donn√©es
make data-tag
cat data-tag

# 3. Lancer le traitement si nouvelle version
make full
```

---

## üìù Commandes disponibles

### Gestion de l'environnement

| Commande | Description |
|----------|-------------|
| [`make config`](Makefile:65) | Configuration initiale et v√©rification des pr√©requis |
| [`make dev`](Makefile:148) | Lance l'environnement complet (Elasticsearch + Backend + Frontend) |
| [`make dev-stop`](Makefile:152) | Arr√™te tous les services de d√©veloppement |
| [`make up`](Makefile:157) | Lance Elasticsearch et le backend uniquement (sans frontend) |
| [`make down`](Makefile:266) | Arr√™te tous les services |
| [`make clean`](Makefile:271) | Nettoyage complet (‚ö†Ô∏è supprime tout) |

### Traitement des donn√©es

| Commande | Description | Dur√©e estim√©e |
|----------|-------------|---------------|
| [`make all`](Makefile:286) | ‚ú® Traitement complet automatique | 1h30 - 10h |
| [`make full`](Makefile:183) | Traitement avec v√©rifications pr√©alables | 1h - 8h |
| [`make recipe-run`](Makefile:161) | Ex√©cution de la recette de traitement | 1h - 8h |
| [`make watch-run`](Makefile:191) | üëÄ Surveillance du traitement en cours | Variable |
| [`make backend-clean-logs`](Makefile:188) | Nettoyage des logs de traitement | Instantan√© |

### Synchronisation des donn√©es

| Commande | Description |
|----------|-------------|
| [`make datagouv-to-storage`](Makefile:70) | üì• Synchronisation depuis data.gouv.fr vers S3 |
| [`make datagouv-to-s3`](Makefile:76) | Alias pour `datagouv-to-storage` |
| [`make datagouv-to-upload`](Makefile:79) | T√©l√©chargement local dans `backend/upload/` |
| [`make data-tag`](Makefile:90) | G√©n√©ration du tag de version des donn√©es |

### Sauvegarde et restauration

| Commande | Description | M√©thode |
|----------|-------------|---------|
| [`make repository-push`](Makefile:249) | üíæ Sauvegarde via repository ES (recommand√©) | Repository |
| [`make repository-restore`](Makefile:212) | ‚ôªÔ∏è Restauration depuis repository ES | Repository |
| [`make repository-check`](Makefile:122) | V√©rification des snapshots disponibles | Repository |
| [`make repository-config`](Makefile:108) | Configuration du repository S3 | Repository |
| [`make backup`](Makefile:225) | Sauvegarde classique (tar) | Backup |
| [`make backup-push`](Makefile:237) | Envoi du backup vers S3 | Backup |
| [`make backup-restore`](Makefile:206) | Restauration depuis backup classique | Backup |
| [`make backup-check`](Makefile:106) | V√©rification des backups classiques | Backup |

**üí° Recommandation** : Utilisez la m√©thode `repository` qui est plus rapide et efficace pour les gros volumes.

### D√©ploiement distant (Cloud)

| Commande | Description |
|----------|-------------|
| [`make remote-config`](Makefile:291) | Configuration du serveur distant |
| [`make remote-deploy`](Makefile:298) | D√©ploiement sur le serveur distant |
| [`make remote-step1`](Makefile:304) | Ex√©cution du traitement √† distance |
| [`make remote-watch`](Makefile:312) | Surveillance du traitement distant |
| [`make remote-step2`](Makefile:318) | Sauvegarde √† distance |
| [`make remote-clean`](Makefile:325) | Nettoyage du serveur distant |
| [`make remote-all`](Makefile:331) | Workflow complet distant |

### Utilitaires

| Commande | Description |
|----------|-------------|
| [`make dataprep-version`](Makefile:92) | Affiche la version du dataprep (hash) |

---

## ‚öôÔ∏è Configuration

### Variables d'environnement principales

#### Configuration Elasticsearch

| Variable | Valeur par d√©faut | Description |
|----------|-------------------|-------------|
| [`ES_INDEX`](Makefile:15) | `deces` | Nom de l'index Elasticsearch |
| [`ES_NODES`](Makefile:16) | `1` | Nombre de n≈ìuds Elasticsearch |
| [`ES_MEM`](Makefile:17) | `1024m` | M√©moire allou√©e √† Elasticsearch |
| [`ES_VERSION`](Makefile:18) | `8.6.1` | Version d'Elasticsearch |
| [`ES_PRELOAD`](Makefile:20) | `[]` | Fichiers √† pr√©charger en m√©moire |

#### Configuration du traitement

| Variable | Valeur par d√©faut | Description |
|----------|-------------------|-------------|
| [`RECIPE`](Makefile:22) | `deces_dataprep` | Nom de la recette √† ex√©cuter |
| [`CHUNK_SIZE`](Makefile:21) | `10000` | Taille des lots de traitement |
| [`RECIPE_THREADS`](Makefile:23) | `4` | Threads pour le traitement des donn√©es |
| [`RECIPE_QUEUE`](Makefile:24) | `1` | Longueur de la queue d'√©criture |
| [`ES_THREADS`](Makefile:25) | `2` | Threads pour l'indexation Elasticsearch |
| [`TIMEOUT`](Makefile:26) | `2520` | Timeout en secondes (42 minutes) |
| [`ERR_MAX`](Makefile:19) | `20` | Nombre max d'erreurs tol√©r√©es |

#### Configuration S3

| Variable | Valeur par d√©faut | Description |
|----------|-------------------|-------------|
| [`STORAGE_BUCKET`](Makefile:30) | `fichier-des-personnes-decedees` | Bucket de stockage des donn√©es |
| [`REPOSITORY_BUCKET`](Makefile:31) | `${STORAGE_BUCKET}-elasticsearch` | Bucket pour les snapshots ES |
| [`DATAGOUV_CONNECTOR`](Makefile:29) | `s3` | Type de connecteur (s3/upload) |
| `STORAGE_ACCESS_KEY` | - | Cl√© d'acc√®s S3 (si n√©cessaire) |
| `STORAGE_SECRET_KEY` | - | Cl√© secr√®te S3 (si n√©cessaire) |

#### Configuration des fichiers

| Variable | Valeur | Description |
|----------|--------|-------------|
| [`FILES_TO_SYNC`](Makefile:39) | `fichier-opposition-deces-.*.csv(.gz)?|deces-.*.txt(.gz)?` | Fichiers √† synchroniser |
| [`FILES_TO_SYNC_FORCE`](Makefile:40) | `fichier-opposition-deces-.*.csv(.gz)?` | Fichiers forc√©s (oppositions RGPD) |
| [`FILES_TO_PROCESS`](Makefile:42) | `deces-([0-9]{4}|2025-m[0-9]{2}).txt.gz` | Fichiers √† traiter |

#### Configuration Cloud (Scaleway)

| Variable | Valeur par d√©faut | Description |
|----------|-------------------|-------------|
| [`SCW_FLAVOR`](Makefile:57) | `PRO2-M` | Type d'instance Scaleway |
| [`SCW_VOLUME_TYPE`](Makefile:58) | `sbs_15k` | Type de volume |
| [`SCW_VOLUME_SIZE`](Makefile:59) | `50000000000` | Taille du volume (50 GB) |
| [`SCW_IMAGE_ID`](Makefile:60) | `8e7f9833...` | ID de l'image de base |

### Exemples de configuration

#### Configuration haute performance

```bash
export ES_MEM=4096m
export ES_NODES=2
export RECIPE_THREADS=8
export ES_THREADS=4
export CHUNK_SIZE=20000

make recipe-run
```

#### Configuration √©conomique (ressources limit√©es)

```bash
export ES_MEM=512m
export RECIPE_THREADS=2
export ES_THREADS=1
export CHUNK_SIZE=5000

make recipe-run
```

#### Configuration avec S3 externe

```bash
export DATAGOUV_CONNECTOR=s3
export STORAGE_ACCESS_KEY=VOTRE_CLE
export STORAGE_SECRET_KEY=VOTRE_SECRET
export STORAGE_BUCKET=mon-bucket-deces
export REPOSITORY_BUCKET=mon-bucket-deces-elasticsearch

make full
```

#### Configuration avec stockage local

```bash
export DATAGOUV_CONNECTOR=upload

# Les fichiers seront t√©l√©charg√©s dans backend/upload/
make datagouv-to-upload
make recipe-run
```

### Fichiers de configuration

| Fichier | Description |
|---------|-------------|
| [`Makefile`](Makefile:1) | Configuration principale et orchestration |
| [`artifacts`](Makefile:63) | Variables d'environnement persist√©es |
| [`docker-compose-local.yml`](docker-compose-local.yml:1) | Configuration Docker locale |
| [`projects/deces-dataprep/recipes/deces_dataprep.yml`](projects/deces-dataprep/recipes/deces_dataprep.yml:1) | Recette de transformation |
| [`projects/deces-dataprep/datasets/deces_src.yml`](projects/deces-dataprep/datasets/deces_src.yml:1) | Configuration source |
| [`projects/deces-dataprep/datasets/deces_index.yml`](projects/deces-dataprep/datasets/deces_index.yml:1) | Mapping Elasticsearch |

---

## üî¨ Traitements des donn√©es

### Format source

Les donn√©es sources sont des fichiers texte √† largeur fixe ([`deces_src.yml`](projects/deces-dataprep/datasets/deces_src.yml:1)) :

| Position | Largeur | Champ | Description |
|----------|---------|-------|-------------|
| 0-80 | 80 | `NOM_PRENOMS` | Nom et pr√©noms (format : `NOM*PRENOMS/`) |
| 80-81 | 1 | `SEXE` | Sexe (1=M, 2=F) |
| 81-89 | 8 | `DATE_NAISSANCE` | Date de naissance (YYYYMMDD) |
| 89-94 | 5 | `CODE_INSEE_NAISSANCE` | Code INSEE commune naissance |
| 94-124 | 30 | `COMMUNE_NAISSANCE` | Libell√© commune de naissance |
| 124-154 | 30 | `PAYS_NAISSANCE` | Libell√© pays de naissance |
| 154-162 | 8 | `DATE_DECES` | Date de d√©c√®s (YYYYMMDD) |
| 162-167 | 5 | `CODE_INSEE_DECES` | Code INSEE commune d√©c√®s |
| 167-177 | 10 | `NUM_DECES` | Num√©ro d'acte de d√©c√®s |

### Pipeline de transformation

La recette [`deces_dataprep.yml`](projects/deces-dataprep/recipes/deces_dataprep.yml:1) applique les transformations suivantes :

#### 1. Identification et normalisation initiale

- G√©n√©ration d'un identifiant unique (`UID`) bas√© sur Blake3
- Normalisation des caract√®res sp√©ciaux
- Extraction du nom et des pr√©noms

#### 2. Filtrage RGPD

- Jointure avec le fichier des oppositions RGPD
- Suppression des enregistrements avec opposition
- Conservation de la confidentialit√©

#### 3. Traitement des dates

- Validation et correction des dates invalides
- Conversion au format standard (`YYYY/MM/DD`)
- Gestion des cas limites (29 f√©vrier, jours > 31, etc.)
- Calcul de l'√¢ge au d√©c√®s

```python
# Exemple de correction de date
20250231 ‚Üí 20250301  # 31 f√©vrier ‚Üí 1er mars
19001301 ‚Üí 19011201  # mois 13 ‚Üí d√©cembre suivant
```

#### 4. Enrichissement g√©ographique

##### Naissance

- Mapping des codes INSEE historiques (fusions de communes)
- Jointure avec le r√©f√©rentiel des communes fran√ßaises
- Ajout des coordonn√©es GPS (`GEOPOINT_NAISSANCE`)
- Gestion des anciennes colonies :
  - Alg√©rie ‚Üí Code pays 99352
  - Mayotte ‚Üí Conversion 985XX ‚Üí 976XX
  - Autres colonies ‚Üí Mapping vers pays actuels

##### D√©c√®s

- M√™me enrichissement que pour la naissance
- Historique des codes INSEE
- G√©olocalisation

#### 5. Normalisation finale

- Mise en forme des noms/pr√©noms (Title Case)
- Cr√©ation de champs de recherche optimis√©s :
  - `PRENOMS_NOM` : Pr√©noms + Nom (lowercase)
  - `PRENOM_NOM` : Premier pr√©nom + Nom (lowercase)
- Conversion du sexe (2 ‚Üí F, 1 ‚Üí M)

### Champs produits

L'index Elasticsearch final ([`deces_index.yml`](projects/deces-dataprep/datasets/deces_index.yml:1)) contient :

#### Identification

- `UID` : Identifiant unique (12 caract√®res)
- `SOURCE` : Nom du fichier source
- `SOURCE_LINE` : Num√©ro de ligne dans le fichier source

#### Identit√©

- `NOM`, `PRENOM`, `PRENOMS` : Identit√© (texte + keyword)
- `PRENOMS_NOM`, `PRENOM_NOM` : Champs de recherche optimis√©s
- `SEXE` : M ou F

#### Naissance

- `DATE_NAISSANCE` : Date brute
- `DATE_NAISSANCE_NORM` : Date normalis√©e (format date)
- `CODE_INSEE_NAISSANCE` : Code INSEE actuel
- `CODE_INSEE_NAISSANCE_HISTORIQUE` : Codes historiques
- `COMMUNE_NAISSANCE` : Libell√©(s) de la commune
- `CODE_POSTAL_NAISSANCE` : Code(s) postal
- `DEPARTEMENT_NAISSANCE` : Code d√©partement
- `PAYS_NAISSANCE` : Pays (texte + keyword)
- `PAYS_NAISSANCE_CODEISO3` : Code ISO3 du pays
- `GEOPOINT_NAISSANCE` : Coordonn√©es GPS (geo_point)

#### D√©c√®s

- `DATE_DECES` : Date brute
- `DATE_DECES_NORM` : Date normalis√©e (format date)
- `AGE_DECES` : √Çge au d√©c√®s (en ann√©es)
- `CODE_INSEE_DECES` : Code INSEE actuel
- `CODE_INSEE_DECES_HISTORIQUE` : Codes historiques
- `COMMUNE_DECES` : Libell√©(s) de la commune
- `CODE_POSTAL_DECES` : Code(s) postal
- `DEPARTEMENT_DECES` : Code d√©partement
- `PAYS_DECES` : Pays (texte + keyword)
- `PAYS_DECES_CODEISO3` : Code ISO3 du pays
- `GEOPOINT_DECES` : Coordonn√©es GPS (geo_point)
- `NUM_DECES` : Num√©ro d'acte (9 caract√®res)

### Analyseurs Elasticsearch

L'index utilise des analyseurs personnalis√©s pour optimiser la recherche :

- **`norm`** : Normalisation (suppression accents, lowercase, espaces)
- **`autocomplete_analyzer`** : Edge n-grams pour l'autocompl√©tion (2-10 caract√®res)
- **Index prefixes** : Sur les dates pour recherches partielles (YYYY, YYYYMM)

---

## ‚òÅÔ∏è D√©ploiement distant

### Workflow distant complet

```bash
# 1. Configuration du serveur distant (Scaleway)
make remote-config

# 2. D√©ploiement sur le serveur
make remote-deploy

# 3. Ex√©cution du traitement √† distance
make remote-step1

# 4. Surveillance du traitement
make remote-watch

# 5. Sauvegarde
make remote-step2

# 6. Nettoyage du serveur
make remote-clean
```

Ou en une seule commande :

```bash
make remote-all
```

### Configuration requise

```bash
# Variables Scaleway
export SCW_FLAVOR=PRO2-M              # Type d'instance
export SCW_VOLUME_SIZE=50000000000    # 50 GB
export SCW_VOLUME_TYPE=sbs_15k        # SSD haute performance

# Variables S3
export STORAGE_ACCESS_KEY=votre_cle
export STORAGE_SECRET_KEY=votre_secret

# Lancer le d√©ploiement
make remote-all
```

### Mise √† jour de l'image de base

Pour cr√©er une nouvelle image de base avec les d√©pendances pr√©charg√©es :

```bash
make update-base-image
```

Cette commande :
1. D√©ploie une instance
2. Met √† jour les paquets syst√®me
3. Pr√©charge les images Docker (Python, Elasticsearch)
4. Cr√©e un snapshot Scaleway
5. Met √† jour le [`SCW_IMAGE_ID`](Makefile:60) dans le Makefile

---

## üîß D√©pannage

### Probl√®mes courants

#### Erreur de m√©moire Elasticsearch

**Sympt√¥me** : `OutOfMemoryError` dans les logs Elasticsearch

**Solution** :
```bash
# Augmenter la m√©moire allou√©e
make recipe-run ES_MEM=2048m

# Ou modifier les limites Docker
# Dans backend/docker-compose.yml, section elasticsearch.mem_limit
```

#### Timeout de traitement

**Sympt√¥me** : Le traitement s'arr√™te avant la fin

**Solution** :
```bash
# Augmenter le timeout (en secondes)
make watch-run TIMEOUT=7200  # 2 heures
```

#### Trop d'erreurs dans le traitement

**Sympt√¥me** : Message "Ooops count exceeds ERR_MAX"

**Solution** :
```bash
# V√©rifier les logs
ls -la backend/log/*deces_dataprep*

# Augmenter le seuil d'erreurs
make recipe-run ERR_MAX=50

# Nettoyer et relancer
make backend-clean-logs
make recipe-run
```

#### Port d√©j√† utilis√©

**Sympt√¥me** : Erreur "port 8081/9200 already in use"

**Solution** :
```bash
# V√©rifier les processus sur les ports
netstat -tlnp | grep -E ':(8081|9200|5000)'

# Arr√™ter les services existants
make down

# Ou tuer le processus sp√©cifique
sudo kill -9 $(lsof -ti:8081)
```

#### Probl√®me de permissions

**Sympt√¥me** : Erreur "Permission denied" lors du nettoyage

**Solution** :
```bash
# Le nettoyage n√©cessite sudo
sudo make clean

# Ou changer les permissions des fichiers Elasticsearch
sudo chown -R $USER:$USER backend/
```

#### Backup/Repository S3 √©choue

**Sympt√¥me** : Erreur lors de la sauvegarde vers S3

**Solution** :
```bash
# V√©rifier les credentials
echo $STORAGE_ACCESS_KEY
echo $STORAGE_SECRET_KEY

# Reconfigurer le repository
rm repository-config
make repository-config

# V√©rifier la connexion S3
make repository-check
```

#### Donn√©es corrompues

**Sympt√¥me** : Donn√©es manquantes ou incorrectes apr√®s traitement

**Solution** :
```bash
# Nettoyer compl√®tement
make clean

# Resynchroniser les donn√©es
make datagouv-to-storage

# V√©rifier la version
make data-tag
cat data-tag

# Relancer le traitement
make all
```

### V√©rifications de sant√©

```bash
# 1. V√©rifier l'√©tat d'Elasticsearch
curl -s http://localhost:9200/_cat/health

# 2. V√©rifier le nombre de documents index√©s
curl -s http://localhost:9200/_cat/indices | grep deces

# 3. V√©rifier les logs de traitement
tail -f backend/log/*deces_dataprep*.log

# 4. V√©rifier l'utilisation des ressources
docker stats

# 5. Tester une recherche simple
curl -X GET "http://localhost:9200/deces/_search?q=NOM:MARTIN&size=1&pretty"
```

### Logs et diagnostic

```bash
# Logs Elasticsearch
docker logs matchid-elasticsearch

# Logs Backend
docker logs matchid-backend

# Logs Frontend
docker logs matchid-frontend

# Logs de la recette
ls -lh backend/log/
cat backend/log/*deces_dataprep*.log
```

### Nettoyage progressif

```bash
# Nettoyage l√©ger (logs uniquement)
make backend-clean-logs

# Nettoyage moyen (arr√™t services + donn√©es temporaires)
make down
rm -f recipe-run full watch-run backup*

# Nettoyage complet (‚ö†Ô∏è supprime tout)
sudo make clean
```

---

## üìö Documentation compl√©mentaire

### Ressources matchID

- [Documentation officielle matchID](https://matchid-project.github.io/)
- [Backend matchID](https://github.com/matchid-project/backend)
- [Frontend matchID](https://github.com/matchid-project/frontend)

### Donn√©es INSEE

- [Fichier des personnes d√©c√©d√©es](https://www.data.gouv.fr/fr/datasets/fichier-des-personnes-decedees/)
- [Documentation du format](https://www.insee.fr/fr/information/4190491)

### Technologies utilis√©es

- [Elasticsearch 8.6](https://www.elastic.co/guide/en/elasticsearch/reference/8.6/index.html)
- [Pandas](https://pandas.pydata.org/docs/)
- [Docker Compose](https://docs.docker.com/compose/)

---

## üìÑ License

Ce projet est sous licence [GNU Lesser General Public License v3.0](LICENSE).

---

## üí° Conseils et bonnes pratiques

### Performance

- ‚ö° **Multi-threading** : Ajustez [`RECIPE_THREADS`](Makefile:23) et [`ES_THREADS`](Makefile:25) selon vos CPU
- üíæ **M√©moire** : Allouez suffisamment de RAM √† Elasticsearch via [`ES_MEM`](Makefile:17)
- üì¶ **Chunk size** : R√©duisez [`CHUNK_SIZE`](Makefile:21) si vous manquez de m√©moire
- üîÑ **Queue** : Augmentez [`RECIPE_QUEUE`](Makefile:24) pour parall√©liser √©criture/traitement

### S√©curit√©

- üîê **Credentials S3** : Ne jamais committer les cl√©s d'acc√®s
- üîí **Variables d'environnement** : Utilisez un fichier `.env` ou un gestionnaire de secrets
- üõ°Ô∏è **RGPD** : Les oppositions sont automatiquement filtr√©es

### Sauvegarde

- üì∏ **Repository** : Pr√©f√©rez la m√©thode repository pour les gros volumes
- ‚è∞ **Fr√©quence** : Sauvegardez apr√®s chaque traitement complet r√©ussi
- ‚úÖ **V√©rification** : Testez r√©guli√®rement la restauration

### Monitoring

- üìä **watch-run** : Surveillez le traitement en temps r√©el
- üìù **Logs** : Consultez les logs en cas d'erreur
- üîç **Elasticsearch** : V√©rifiez r√©guli√®rement la sant√© du cluster

### D√©veloppement

- üß™ **Tests** : Testez vos modifications sur des √©chantillons via l'interface matchID
- üìö **Documentation** : Documentez vos changements dans la recette
- üîÑ **Version** : La version du dataprep est calcul√©e automatiquement via hash

---

**Version du dataprep** : Calcul√©e automatiquement √† partir du hash des fichiers de configuration ([`DATAPREP_VERSION`](Makefile:3))

Pour obtenir la version actuelle :
```bash
make dataprep-version
